---
title: "Business Intelligence Project"
author: "<Specify your name here>"
date: "<Specify the date when you submitted the lab>"
output:
  github_document: 
    toc: yes
    toc_depth: 4
    fig_width: 6
    fig_height: 4
    df_print: default
editor_options:
  chunk_output_type: console
---

# Student Details

|                                              |     |
|----------------------------------------------|-----|
| **Student ID Number**                        | 112827,132234,134265 |
| **Student Name**                             | Kenneth Mungai,Kelly Noella, Emmanuel Kiptoo |
| **BBIT 4.2 Group**                           | A,B |
| **BI Project Group Name/ID (if applicable)** | Lumin |

# Setup Chunk

**Note:** the following KnitR options have been set as the global defaults: <BR> `knitr::opts_chunk$set(echo = TRUE, warning = FALSE, eval = TRUE, collapse = FALSE, tidy = TRUE)`.

More KnitR options are documented here <https://bookdown.org/yihui/rmarkdown-cookbook/chunk-options.html> and here <https://yihui.org/knitr/options/>.

```{r setup, include=FALSE}
library(formatR)
knitr::opts_chunk$set(
  warning = FALSE,
  collapse = FALSE
)
```

# STEP 1. Install and Load the Required Packages
```{r step-one-chunk}
### caret ----
if (require("caret")) {
  require("caret")
} else {
  install.packages("caret", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

### klaR ----
if (require("klaR")) {
  require("klaR")
} else {
  install.packages("klaR", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

### e1071 ----
if (require("e1071")) {
  require("e1071")
} else {
  install.packages("e1071", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

### readr ----
if (require("readr")) {
  require("readr")
} else {
  install.packages("readr", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

### LiblineaR ----
if (require("LiblineaR")) {
  require("LiblineaR")
} else {
  install.packages("LiblineaR", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

### naivebayes ----
if (require("naivebayes")) {
  require("naivebayes")
} else {
  install.packages("naivebayes", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

### mlbench ----
if (require("mlbench")) {
  require("mlbench")
} else {
  install.packages("mlbench", dependencies = TRUE,
                   repos = "https://cloud.r-project.org")
}

library(readr)
```

# STEP 2. Load the dataset
```{r step-two-chunk}
## STEP 2. Load the dataset ----
data(PimaIndiansDiabetes)
summary(PimaIndiansDiabetes)

# The str() function is used to compactly display the structure (variables
# and data types) of the dataset
str(PimaIndiansDiabetes)
library(readr)
```

## Splitting the dataset into train and test sets
```{r step-three-chunk}
### 1. Split the dataset ----
# Define a 75:25 train:test data split of the dataset.
# 75% to train and 25% to test the model

train_index <- createDataPartition(PimaIndiansDiabetes$diabetes,
                                   p = 0.75,
                                   list = FALSE)
pima_indians_diabetes_train <- PimaIndiansDiabetes[train_index, ]
pima_indians_diabetes_test <- PimaIndiansDiabetes[-train_index, ]

library(readr)
```

# Train the model
```{r step-four-chunk}
#### 2.a OPTION 1: naiveBayes() function in the e1071 package ----

PimaIndiansDiabetes_model_nb_e1071 <- # nolint
  e1071::naiveBayes(diabetes ~ pregnant + glucose + 
                      pressure + triceps +
                      insulin +
                      mass + pedigree +
                      age,
                    data = pima_indians_diabetes_train)
 
#### 2.b. OPTION 2: naiveBayes() function in the caret package ----
PimaIndiansDiabetes_model_nb_caret <- # nolint
  caret::train(diabetes ~ ., data =
                 pima_indians_diabetes_train[, c("pregnant", "age", "glucose",
                                             "pressure", "triceps", "insulin",
                                             "mass",
                                             "pedigree",
                                             "diabetes"
                                             )],
               method = "naive_bayes")

#### 2.c OPTION 3: "NaiveBayes()" function in the "klaR" package ----
PimaIndiansDiabetes_model_nb_klaR <- # nolint
  klaR::NaiveBayes(`diabetes` ~ .,
                   data = pima_indians_diabetes_train)

library(readr)
```

# STEP3. Test the model using the testing dataset
```{r step-five-chunk}
#### 3.a Test the e1071 Naive Bayes model ----

predictions_nb_e1071 <-
  predict(PimaIndiansDiabetes_model_nb_e1071,
          pima_indians_diabetes_test[, c("pregnant", "glucose",
                                     "pressure", "triceps", "insulin",
                                     "mass","pedigree", "age")])

#### 3.b Test the caret Naive Bayes model ----

predictions_nb_caret <-
  predict(PimaIndiansDiabetes_model_nb_caret,
          pima_indians_diabetes_test[, c("pregnant", "glucose",
                                         "pressure", "triceps", "insulin",
                                         "mass","pedigree", "age")])

### 4. Results ----
#### 4.a e1071 Naive Bayes model ----
print(predictions_nb_e1071)
caret::confusionMatrix(predictions_nb_e1071,
                       pima_indians_diabetes_test[, c("pregnant", "glucose",
                                                      "pressure", "triceps",
                                                      "insulin",
                                                      "mass","pedigree", 
                                                      "age",
                                                      "diabetes")]$diabetes)

plot(table(predictions_nb_e1071,
           pima_indians_diabetes_test[, c("pregnant", "glucose",
                                          "pressure", "triceps",
                                          "insulin",
                                          "mass","pedigree", 
                                          "age",
                                          "diabetes")]$diabetes))

#### 4. b caret Naive Bayes model ----
print(PimaIndiansDiabetes_model_nb_caret)
caret::confusionMatrix(predictions_nb_caret,
                       pima_indians_diabetes_test[, c("pregnant", "glucose",
                                                      "pressure", "triceps",
                                                      "insulin",
                                                      "mass","pedigree", 
                                                      "age",
                                                      "diabetes")]$diabetes)

plot(table(predictions_nb_caret,
           pima_indians_diabetes_test[, c("pregnant", "glucose",
                                          "pressure", "triceps",
                                          "insulin",
                                          "mass","pedigree", 
                                          "age",
                                          "diabetes")]$diabetes))
library(readr)
```

# STEP4:Bootstrapping
```{r step-six-chunk}
### 1. Split the dataset ----
# Define a 75:25 train:test data split of the dataset.
# 75% to train and 25% to test the model

train_index <- createDataPartition(PimaIndiansDiabetes$diabetes,
                                   p = 0.75,
                                   list = FALSE)
pima_indians_diabetes_train <- PimaIndiansDiabetes[train_index, ]
pima_indians_diabetes_test <- PimaIndiansDiabetes[-train_index, ]

### 2. Train a  classification model ----
#### 2.a Train control  ----
train_control <- trainControl(method = "cv", number = 10)

PimaIndiansDiabetes_model_logit <- # nolint
  train(`diabetes` ~
                 `pregnant` + `glucose` +
                 `pressure` + `triceps` +
                 `insulin` + `mass` +
                 `pedigree` +
                 `age`,
               data = pima_indians_diabetes_train,
               trControl = train_control,
               na.action = na.omit, method = "glm", family ="binomial", 
        metric = "Accuracy")

### 3. Test the trained linear regression model   ----
predictions_logit <- predict(PimaIndiansDiabetes_model_logit,
                          pima_indians_diabetes_test[, c("pregnant", "glucose",
                                                         "pressure", "triceps",
                                                         "insulin",
                                                         "mass","pedigree", 
                                                         "age",
                                                         "diabetes")])

### 4. Results   ----
print(PimaIndiansDiabetes_model_logit)
print(predictions_logit)
library(readr)
```

# STEP5:CV, Repeated CV, and LOOCV
```{r step-seven-chunk}
### 1. Split the dataset ----
# Define a 75:25 train:test data split of the dataset.
# 75% to train and 25% to test the model

train_index <- createDataPartition(PimaIndiansDiabetes$diabetes,
                                   p = 0.75,
                                   list = FALSE)
pima_indians_diabetes_train <- PimaIndiansDiabetes[train_index, ]
pima_indians_diabetes_test <- PimaIndiansDiabetes[-train_index, ]

### 2. Regression: Linear model      ----
#### 2.a  10-fold cross validation  ----
train_control <- trainControl(method = "cv", number = 10)

PimaIndiansDiabetes_model_lm <- # nolint
  train(`diabetes` ~
          `pregnant` + `glucose` +
          `pressure` + `triceps` +
          `insulin` + `mass` +
          `pedigree` +
          `age`,
        data = pima_indians_diabetes_train,
        trControl = train_control,
        na.action = na.omit, method = "glm", 
        metric = "Accuracy")

#### 2.b Test the trained model   ----
predictions_lm <- predict(PimaIndiansDiabetes_model_lm, pima_indians_diabetes_test[,c("pregnant", "glucose",
                                                                                      "pressure", "triceps",
                                                                                      "insulin",
                                                                                      "mass","pedigree", 
                                                                                      "age",
                                                                                      "diabetes")])

### 2.c. View the RMSE and the predicted values ====
print(PimaIndiansDiabetes_model_lm)
print(predictions_lm)

library(readr)
```

# STEP6: Classification: LDA with k-fold Cross Validation
```{r step-eight-chunk}
#### 3.a LDA classifier based on a 5-fold CV    ----
PimaIndiansDiabetes_model_lda <-
  caret::train(`diabetes` ~ ., data = pima_indians_diabetes_train,
               trControl = train_control, na.action = na.omit, method = "lda2",
               metric = "Accuracy")

#### 3.b Test the trained LDA model   ----
predictions_lda <- predict(PimaIndiansDiabetes_model_lda,
                           pima_indians_diabetes_test[, c("pregnant", "glucose",
                                                          "pressure", "triceps",
                                                          "insulin",
                                                          "mass","pedigree", 
                                                          "age",
                                                          "diabetes")])

#### 3.c. View the summary of the model and view the confusion matrix ----
print(PimaIndiansDiabetes_model_lda)
caret::confusionMatrix(predictions_lda, pima_indians_diabetes_test$diabetes)


library(readr)
```

# STEP7: Classification: Naive Bayes with Repeated k-fold Cross Validation
```{step-nine-chunk}
#### 4.a. Train an e1071::naive Bayes classifier based on the diabetes variable ----
PimaIndiansDiabetes_model_nb <-
  e1071::naiveBayes(`diabetes` ~ ., data = pima_indians_diabetes_train)

#### 4.b. Test the trained naive Bayes classifier   ----
predictions_nb_e1071 <-
  predict(PimaIndiansDiabetes_model_nb, pima_indians_diabetes_test[, 1:9])

#### 4.c. View a summary of the naive Bayes model and the confusion matrix ----
print(PimaIndiansDiabetes_model_nb)
caret::confusionMatrix(predictions_nb_e1071, pima_indians_diabetes_test$diabetes)

### 5. Classification: SVM with Repeated k-fold Cross Validation ----
#### 5.a. SVM Classifier using 5-fold cross validation with 3 reps ----


train_control <- trainControl(method = "repeatedcv", number = 5, repeats = 3)

PimaIndiansDiabetes_model_svm <-
  caret::train(`diabetes` ~ ., data = pima_indians_diabetes_train,
               trControl = train_control, na.action = na.omit,
               method = "svmLinearWeights2", metric = "Accuracy")

#### 5.b. Test the trained SVM model ----
predictions_svm <- predict(PimaIndiansDiabetes_model_svm, pima_indians_diabetes_test[, 1:9])

#### 5.c. View a summary of the model and view the confusion matrix ----
print(PimaIndiansDiabetes_model_svm)
caret::confusionMatrix(predictions_svm, pima_indians_diabetes_test$diabetes)


### 6. Classification: Naive Bayes with Leave One Out Cross Validation ----


#### 6.a. Train a Naive Bayes classifier based on an LOOCV ----
train_control <- trainControl(method = "LOOCV")

PimaIndiansDiabetes_nb_loocv <-
  caret::train(`diabetes` ~ ., data = pima_indians_diabetes_train,
               trControl = train_control, na.action = na.omit,
               method = "naive_bayes", metric = "Accuracy")

#### 6.b. Test the trained model using the testing dataset ----
predictions_nb_loocv <-
  predict(PimaIndiansDiabetes_nb_loocv, pima_indians_diabetes_test[, 1:9])

#### 6.c. View the confusion matrix ----
print(PimaIndiansDiabetes_nb_loocv)
caret::confusionMatrix(predictions_nb_loocv, pima_indians_diabetes_test$diabetes)

library(readr)
```